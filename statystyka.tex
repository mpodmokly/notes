\documentclass[11pt, leqno]{scrartcl}
\usepackage{polski}
\usepackage[polish]{babel}

\usepackage{graphicx, float, caption, subcaption}
\usepackage{tabularx, multirow, hyperref, enumitem}
\usepackage{listings, xcolor}
\usepackage{amsmath, amssymb}
\usepackage{amsthm}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=black,
    citecolor=black
}

\newtheoremstyle{mydefinition}
    {1.2em}{1.2em}{}{}{\bfseries}{}{0pt}
    {\thmname{#1} \thmnumber{#2.}\thmnote{ #3.}\\[0.5em]}
\theoremstyle{mydefinition}
\newtheorem{definition}{Definicja}[subsection]

\newtheoremstyle{mytheorem}
    {1.2em}{1.2em}{}{}{\bfseries}{}{0pt}
    {\thmname{#1} \thmnumber{#2.}\thmnote{ #3.}\\[0.5em]}
\theoremstyle{mytheorem}
\newtheorem{theorem}{Twierdzenie}[subsection]

\makeatletter
\renewcommand{\thebibliography}[1]{%
  \list{\@biblabel{\@arabic\c@enumiv}}%
       {\settowidth\labelwidth{\@biblabel{#1}}%
        \leftmargin\labelwidth
        \advance\leftmargin\labelsep
        \usecounter{enumiv}%
        \let\p@enumiv\@empty
        \renewcommand\theenumiv{\@arabic\c@enumiv}}%
  \sloppy\clubpenalty4000\widowpenalty4000%
  \sfcode`\.=1000\relax}
\makeatother

\graphicspath{{images/}}

\title{Rachunek prawdopodobieństwa i statystyka}
\author{Mateusz Podmokły III rok Informatyka WI}
\date{semestr zimowy 2025}

\begin{document}
    \maketitle
    \tableofcontents
    \newpage

    \section{Podstawy rachunku prawdopodobieństwa}
    \subsection{Przestrzeń probabilistyczna}
    \begin{definition}[Przestrzeń zdarzeń elementarnych]
        Niepusty zbiór $\Omega$ wszystkich możliwych wyników
        doświadczenia losowego. Jego elementy to zdarzenia elementarne.
    \end{definition}
    \begin{definition}[$\sigma$-algebra zdarzeń]
        Podrodzina $\Sigma$ w rodzinie wszystkich podzbiorów $\Omega$
        o następujących właściwościach:
        \begin{enumerate}
            \item $\Omega \in \Sigma$
            \item Jeśli $A \in \Sigma$ to $A'=\Omega \setminus A \in
                \Sigma$
            \item Dla dowolnego ciągu zbiorów $A_1,A_2,\ldots$ takiego,
                że $A_i \in \Sigma$ dla $i \in \mathbb{N}$, zachodzi
                \[
                    \bigcup_{i=1}^{\infty}A_i \in \Sigma
                \]
        \end{enumerate}
        Jej elementy to zdarzenia losowe.
    \end{definition}
    \noindent
    Własności zdarzeń:
    \begin{enumerate}
        \item $\emptyset \in \Sigma$
        \item Jeśli $A_1,A_2,\ldots,A_n \in \Sigma$, to
            $A_1 \cup A_2 \cup \ldots \cup A_n \in \Sigma$
        \item Dla dowolnego ciągu $A_1,A_2,\ldots$ takiego, że
            $A_i \in \Sigma$ dla $i \in \mathbb{N}$, zachodzi
            \[
                \bigcap_{i=1}^{\infty}A_i \in \Sigma
            \]
        \item Jeśli $A,B \in \Sigma$ to $A \setminus B \in \Sigma$
    \end{enumerate}
    Określmy niezbędną terminologię:
    \begin{description}
        \item[$\emptyset$] -- zdarzenie niemożliwe
        \item[$\Omega$] -- zdarzenie pewne
        \item[$A'=\Omega \setminus A$] -- dopełnienie zdarzenia $A$
        \item[$A \cap B= \emptyset$] -- zdarznia wzajemnie się
            wykluczają.
    \end{description}
    \begin{definition}[Miara probabilistyczna (rozkładbprawdopodobieństwa)]
        W przestrzeni $\Omega$ z $\sigma$-algebrą zdarzeń $\Sigma$
        dowolne odwzorowanie
        \[
            P:\Sigma \to [0,1]
        \]
        spełniające warunki:
        \begin{enumerate}
            \item $P(\Omega)=1$
            \item Dla dowolnego ciągu zdarzeń $A_1,A_2,\ldots$ takiego,
                że $A_i \in \Sigma$ dla $i \in \mathbb{N}$ oraz
                $A_i \cap A_j=\emptyset$ dla $i \neq j$ zachodzi
                \[
                    P\left(\bigcup_{i=1}^{\infty}A_i\right)=
                    \sum_{i=1}^{\infty}P(A_i)
                \]
        \end{enumerate}
    \end{definition}
    Własności prawdopodobieństwa:
    \begin{enumerate}
        \item $P(\emptyset)$=0
        \item Jeśli skończony ciąg zdarzeń $A_1,A_2,\ldots,A_n$ spełnia
            warunek
            \[
                A_i \cap A_j = \emptyset \text{ dla }i \neq j
            \]
            to
            \[
                P(A_1 \cup A_2 \cup \ldots \cup A_n)=
                P(A_1)+P(A_2)+\ldots +P(A_n)
            \]
        \item Dla dowolnego zdarzenia $A$ zachodzi
            \[
                P(A')=1-P(A)
            \]
        \item Dla dowolnych zdarzeń $A$ i $B$ zachodzi
            \[
                P(A \cup B)=P(A)+P(B)-P(A \cap B)
            \]
        \item Jeśli $A \subset B$ to $P(A)\leq P(B)$
        \item Jeśli zdarzenia $A_1,A_2,\ldots$ tworzą ciąg
            wstępujący, tzn.
            \[
                A_1 \subset A_2 \subset \ldots
            \]
            to
            \[
                P\left( \bigcup_{i=1}^{\infty}A_i \right)=
                \lim_{i \to \infty}P(A_i)
            \]
        \item Jeśli zdarzenia $A_1,A_2,\ldots$ tworzą ciąg
            zstępujący, tzn.
            \[
                A_1 \supset A_2 \supset \ldots
            \]
            to
            \[
                P\left( \bigcap_{i=1}^{\infty}A_i \right)=
                \lim_{i \to \infty}P(A_i)
            \]
    \end{enumerate}
    \begin{definition}[Przestrzeń probabilistyczna]
        Trójka $(\Omega,\Sigma,P)$, gdzie:
        \begin{description}
            \item[$\Omega$] -- niepusty zbiór,
            \item[$\Sigma$] -- $\sigma$-algebra w $\Omega$,
            \item[$P$] -- miara probabilistyczna.
        \end{description}
    \end{definition}
    Liczbę $P(A)$ nazywamy prawdopodobieństwem zdarzenia $A$.

    \subsection{Prawdopodobieństwo warunkowe}
    \begin{definition}[Prawdopodobieństwo warunkowe]
        Liczba określona wzorem
        \[
            P(A \mid B)=\frac{P(A \cap B)}{P(B)}
        \]
        gdzie
        \begin{description}
            \item[$A,B \subset \Omega$] -- zdarzenia,
            \item[$P(B)>0$.]
        \end{description}
        Jest to prawdopodobieństwo $A$ pod warunkiem $B$.
    \end{definition}
    \begin{definition}[Układ zupełny zdarzeń]
        Skończony lub nieskończony ciąg zdarzeń $A_1,A_2,\ldots$ jeśli
        zdarzenia w ciągu parami wzajemnie się wykluczają, tzn.
        \[
            A_i \cap A_j =\emptyset, \quad i \neq j
        \]
        oraz zachodzi
        \[
            \Omega=\bigcup_{i}A_i
        \]
    \end{definition}
    \begin{theorem}[Twierdzenie o prawdopodobieństwie całkowitym]
        Jeśli zdarzenia $A_1,A_2,\ldots$ tworzą układ zupełny oraz
        $P(A_i)>0$ dla $i \in \mathbb{N}$, to dla dowolnego
        zdarzenia $B$ zachodzi
        \[
            P(B)=\sum_{i}P(B \mid A_i)P(A_i)
        \]
    \end{theorem}
    \begin{theorem}[Twierdzenie Bayesa]
        Jeśli zdarzenia $A_i$ tworzą układ zupełny taki, że
        $P(A_i)>0$ dla $i \in \mathbb{N}$, a $B$ jest zdarzeniem
        takim, że $P(B)>0$, to dla dowolnego $k$ zachodzi
        \[
            P(A_k \mid B)=\frac{P(B \mid A_k)P(A_k)}
            {\sum_{i}P(B \mid A_i)P(A_i)}
        \]
    \end{theorem}
    
    \subsection{Niezależność zdarzeń}
    \begin{definition}[Niezależność zdarzeń]
        Zdarzenia $A$ i $B$ nazywamy niezależnymi jeśli
        \[
            P(A,B)=P(A) \cdot P(B)
        \]
        Zdarzenia $A_1,A_2,\ldots,A_k$ nazywamy niezależnymi jeśli dla
        każdego układu indeksów $i_1,i_2,\ldots,i_k$ oraz dla każdego
        $k \in \{1,2,\ldots,m\}$ zachodzi
        \[
            P(A_{i_1},A_{i_2},\ldots,A_{i_k})=P(A_{i_1}) \cdot
            P(A_{i_2}) \cdot \ldots \cdot P(A_{i_k})
        \]
    \end{definition}
    \begin{definition}[Niezależność warunkowa]
        Zdarzenia $A$ i $B$ są warunkowo niezależne względem $C$
        dla $P(C)>0$ jeśli
        \[
            P(A,B \mid C)=P(A \mid C) \cdot P(B \mid C)
        \]
        Zdarzenia $A_1,A_2,\ldots,A_k$ są warunkowo niezależne względem $C$ dla
        $P(C)>0$ jeśli dla każdego układu indeksów $i_1,i_2,\ldots,i_k$ oraz dla
        każdego $k \in \{1,2,\ldots,m\}$ zachodzi
        \[
            P(A_{i_1},A_{i_2},\ldots,A_{i_k} \mid C)=P(A_{i_1} \mid C) \cdot
            P(A_{i_2} \mid C) \cdot \ldots \cdot P(A_{i_k} \mid C)
        \]
    \end{definition}

    \section{Zmienne losowe jednowymiarowe}
    \subsection{Zmienna losowa}
    \begin{definition}[Zmienna losowa]
        Zmienna losowa to odwzorowanie
        \[
            X:\Omega \to \mathcal{X}
        \]
        takie, że dla każdego $A \in \Sigma_{\mathcal{X}}$ zachodzi
        \[
            X^{-1}(A) \in \Sigma
        \]
        gdzie $(\Omega,\Sigma,P)$ jest przestrzenią probabilistyczną,
        $\mathcal{X}$ dowolnym niepustym zbiorem, a $\Sigma_{\mathcal{X}}$
        $\sigma$-algebrą podzbiorów $\mathcal{X}$.
    \end{definition}
    \begin{definition}[Rozkład prawdopodobieństwa zmiennej losowej]
        Funkcję
        \[
            P_X:\Sigma_{\mathcal{X}} \to [0,1]
        \]
        określoną następująco
        \[
            P_X(A)=P(X^{-1}(A))
        \]
        gdzie $(\Omega,\Sigma,P)$ jest przestrzenią probabilistyczną,
        $\mathcal{X}$ niepustym zbiorem, $\Sigma_{\mathcal{X}}$
        $\sigma$-algebrą w $\mathcal{X}$, a $X:\Omega \to \mathcal{X}$
        zmienną losową.
    \end{definition}

    \subsection{Zmienne losowe rzeczywiste}
    \begin{definition}[Dystrybuanta zmiennej losowej]
        Niech $X:\Omega \to \mathbb{R}$ będzie zmienną losową rzeczywistą.
        Dystrybuantą zmiennej losowej rzeczywistej $X$ nazywamy funkcję
        \[
            F_X:\mathbb{R} \to [0,1]
        \]
        określoną wzorem
        \[
            F_X(x)=P(X \leq x)=P_X((-\infty,x])
        \]
    \end{definition}
    \noindent
    Własności dystrybuanty:
    \begin{enumerate}
        \item Jeśli $a<b$, to $F_X(b)-F_X(a)=P(a<X \leq b)$
        \item $F_X$ jest niemalejąca
        \item $\lim\limits_{x \to -\infty}F_X(x)=0$
            i $\lim\limits_{x \to +\infty}F_X(x)=1$
        \item $F_X$ jest prawostronnie ciągła, tzn. dla dowolnego
            $x_0 \in \mathbb{R}$ zachodzi
            \[
                \lim_{x \to x_0^+}F_X(x)=F_X(x_0)
            \]
        \item $F_X(x_0-)=\lim\limits_{x \to x_0^-}F_X(x)=P(X<x_0)$
        \item $P(X=x)=F_X(x)-F_X(x-)$
        \item $F_X$ jest ciągła w $x_0 \in \mathbb{R}$ wtedy i tylko
            wtedy, gdy
            \[
                P(X=x_0)=0
            \]
    \end{enumerate}
    \begin{definition}[Funkcja prawdopodobieństwa]
        Rozkład dyskretny zmiennej $X$ jest wyznaczony przez funkcję
        \[
            p:\mathcal{S} \to [0,1]
        \]
        określoną następująco:
        \[
            p(x_k)=P_X(x_k)=P(X=x_k)
        \]
        Funkcję $p$ nazywamy funkcją prawdopodobieństwa rozkładu zmiennej $X$.
    \end{definition}
    \begin{definition}[Funkcja gęstości]
        Gęstość zmiennej losowej $X$ to funkcja
        \[
            f:\mathbb{R} \to [0,+\infty)
        \]
        taka, że dla dowolnych $a,b\in \mathbb{R} \cup \{-\infty,+\infty\}$
        takich, że $a<b$ zachodzi
        \[
            P(a<X<b)=\int_{a}^{b}f(x)dx
        \]
    \end{definition}

    \subsection{Parametry zmiennej losowej}
    \begin{definition}[Wartość oczekiwana]
        Wartością oczekiwaną zmiennej losowej rzeczywistej $X$ o rozkładzie
        dyskretnym z funkcją prawdopodobieństwa $p$ nazywamy liczbę określoną
        wzorem
        \[
            \mu=\mathbb{E}(X)=\sum_{k:x_k \in \mathcal{S}}
            x_k \cdot p(x_k)
        \]
        Jeśli $X$ jest zmienną o rozkładzie ciągłym z gęstością $f$, to
        \[
            \mu =\int_{-\infty}^{\infty}xf(x)dx
        \]
    \end{definition}
    \noindent
    Własności wartości oczekiwanej:
    \begin{enumerate}
        \item $\mathbb{E}c=c$
        \item $\mathbb{E}(aX+bY+c)=a\mathbb{E}X+b\mathbb{E}Y+c$
    \end{enumerate}
    \begin{definition}[Moment zwykły]
        Momentem zwykłym rzędu $k$ zmiennej losowej $X$ dla
        $k \in \mathbb{N} \setminus \{0\}$ nazywamy liczbę
        \[
            \mu'_k=\mathbb{E}(X^k)
        \]
    \end{definition}
    \begin{definition}[Moment centralny]
        Momentem centralnym rzędu $k$ zmiennej losowej $X$ nazywamy liczbę
        \[
            \mu_k=\mathbb{E}((X-\mu)^k)
        \]
    \end{definition}
    \noindent
    Dla rozkładów dyskretnych
    \[
        \mu'_k=\sum_{i:x_i\in \mathcal{S}}x_i^k \cdot p(x_i)
    \]
    \[
        \mu_k=\sum_{i:x_i\in \mathcal{S}}(x_i-\mu)^k \cdot p(x_i)
    \]
    Dla rozkładów ciągłych
    \[
        \mu'_k=\int_{-\infty}^{\infty}x^kf(x)dx
    \]
    \[
        \mu_k=\int_{-\infty}^{\infty}(x-\mu)^kf(x)dx
    \]
    \begin{definition}[Wariancja]
        Wariancją zmiennej losowej $X$ nazywamy jej drugi moment centralny, tzn.
        \[
            Var(X)=\sigma^2=\mu_2=\mathbb{E}((X-\mu)^2)
        \]
    \end{definition}
    \noindent
    Własności wariancji:
    \begin{enumerate}
        \item $Var(X)=\mathbb{E}(X^2)-(\mathbb{E}X)^2$
        \item Jeśli zmienna $X$ ma skończoną wariancję, to dla dowolnych
            $a,b\in \mathbb{R}$ zachodzi
            \[
                Var(aX+b)=a^2Var(X)
            \]
        \item $Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$
        \item $Var(X)=0$ wtedy i tylko wtedy, gdy $X$ jest stała
            z prawdopodobieństwem 1, tzn. istnieje $x_0\in \mathbb{R}$
            takie, że
            \[
                P(X \neq x_0)=0
            \]
    \end{enumerate}
    \begin{definition}[Odchylenie standardowe]
        Odchyleniem standardowym zmiennej losowej $X$ nazywamy pierwiastek jej
        wariancji, tzn.
        \[
            \sigma=\sqrt{Var(X)}
        \]
    \end{definition}
    \begin{definition}[Funkcja tworząca momenty (MGF)]
        Funkjcą tworzącą momenty rzeczywistej zmiennej losowej $X$
        nazywa się funkcję określoną wzorem
        \[
            M_X(t)=\mathbb{E}\left(e^{tX}\right)
        \]
    \end{definition}
    Jeśli $X$ ma rozkład dyskretny z funkcją prawdopodobieństwa $p$, to funkcja
    tworząca momenty wyraża się wzorem
    \[
        M_X(t)=\sum_{x_k\in \mathcal{S}}e^{tx_k}p(x_k)
    \]
    Jeśli $X$ ma rozkład ciągły o gęstości $f$, to funkcja tworząca momenty ma
    postać
    \[
        M_X(t)=\int_{-\infty}^{+\infty}e^{tx}f(x)dx
    \]
    Niech $a,b \in \mathbb{R}$. Własności MGF:
    \begin{enumerate}
        \item $M_{aX}(t)=M_X(at)$
        \item $M_{X+b}(t)=e^{bt}M_X(t)$
        \item $M_{aX+b}(t)=e^{bt}M_X(at)$
        \item $M^{(k)}(0)=\mathbb{E}(X^k)$
    \end{enumerate}
    \begin{definition}[Współczynnik asymetrii (skośność)]
        Współczynnikiem skośności rozkładu zmiennej $X$ nazywamy liczbę
        \[
            A=\gamma_1=\frac{\mu_3}{\sigma^3}
        \]
        Rozkład, dla którego:
        \begin{itemize}
            \item $A=0$ nazywa się \textbf{symetrycznym}
            \item $A>0$ nazywa się \textbf{prawostronnie skośnym}
            \item $A<0$ nazywa się \textbf{lewostronnie skośnym}
        \end{itemize}
    \end{definition}
    \begin{definition}[Kurtoza]
        Kurtozą nazywamy liczbę
        \[
            Kurt(X)=K=\frac{\mu_4}{\sigma^4}
        \]
        natomiast \textbf{kurtozą nadwyżkową} nazywamy liczbę
        \[
            \gamma_3=Kurt(X)-3
        \]
    \end{definition}
    \begin{definition}[Standaryzacja]
        Zmienną o wartości średniej 0 i wariancji 1 nazywa się zmienną
        standaryzowaną. Jeśli $X$ jest dowolną zmienną o niezerowej wariancji, to
        \[
            Z=\frac{X-\mu}{\sigma}
        \]
        jest zmienną standaryzowaną.
    \end{definition}
    \begin{theorem}[Nierówność Czebyszewa]
        Jeśli zmienna losowa $X$ ma skończoną wartość średnią $\mu$ i skończoną
        wariancję $\sigma^2$, to dla dowolnego $\epsilon>0$ zachodzi
        \[
            P(|X-\mu|\geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}
        \]
        Jeśli w miejsce $\epsilon$ podstawimy $\epsilon \sigma$, to otrzymamy
        \[
            P(|X-\mu|\geq \epsilon \sigma) \leq \frac{1}{\epsilon^2}
        \]
    \end{theorem}
    \begin{definition}[Kwantyl]
        Kwantylem rzędu $p\in (0,1)$ zmiennej losowej $X$ o dystrybuancie
        $F$ nazywamy dowolną liczbę $q_p \in \mathbb{R}$ taką, że
        \[
            F(q_p-) \leq p \leq F(q_p)
        \]
        Kwantyl $q_{0.5}$ rzędu $\frac{1}{2}$ nazywamy \textbf{medianą}, kwantyl
        rzędu $\frac{1}{4}$ nazywamy \textbf{dolnym kwartylem}, a kwantyl rzędu
        $\frac{3}{4}$ nazywamy \textbf{górnym kwartylem}.
    \end{definition}
    Jeśli $X$ ma rozkład ciągły, to kwantylem $q_p$ rzędu $p$ jest dowolne
    $q_p$ spełniające równanie
    \[
        F(q_p)=p
    \]
    \begin{definition}[Moda]
        Modą zmiennej losowej o rozkładzie dyskretnym nazywa się dowolne
        maksimum funkcji prawdopodobieństwa tego rozkładu. Jeżeli zmienna ma
        rozkład ciągły to modą jest dowolne maksimum lokalne gęstości tego
        rozkładu.
    \end{definition}
    
    \section{Wybrane rozkłady prawdopodobieństwa}
    \subsection{Rozkłady dyskretne}
    \begin{definition}[Rozkład jednopunktowy]
        Jeśli $\mathcal{S}=\{x_0\}$ i $p(x_0)=1$, to mówimy, że zmienna losowa
        ma \textbf{rozkład jednopunktowy}. Wtedy przyjmuje parametry:
        \[
            \mu=x_0
        \]
        \[
            \sigma^2=0
        \]
        \[
            A=0
        \]
        \[
            K=0
        \]
        \[
            \gamma_2=-3
        \]
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{jednopunktowy.png}
            \caption{Funkcja prawdopodobieństwa w rozkładzie
                jednopunktowym.}
        \end{figure}
    \end{definition}
    \begin{definition}[Rozkład dwupunktowy]
        Jeśli
        \[
            \mathcal{S}=\{x_1,x_2\}
        \]
        oraz
        \[
            p(x_1)=\theta
        \]
        \[
            p(x_2)=1-\theta
        \]
        dla pewnego $\theta \in (0,1)$, to mówimy, że zmienna losowa ma
        \textbf{rozkład dwupunktowy} z parametrem $\theta$. Wtedy
        przyjmuje parametry:
        \[
            \mu=\theta x_1+(1-\theta)x_2
        \]
        \[
            \sigma^2=\theta(1-\theta)(x_1-x_2)^2
        \]
        \[
            A=sgn(x_1-x_2)\frac{1-2\theta}{\sqrt{\theta(1-\theta)}}
        \]
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{dwupunktowy.png}
            \caption{Funkcja prawdopodobieństwa w rozkładzie
                dwupunktowym.}
        \end{figure}
    \end{definition}
    \begin{definition}[Rozkład Bernoulli'ego]
        Rozkład dwupunktowy, w którym $x_1=1$ i $x_2=0$ nazywa się
        \textbf{rozkładem Bernoulli'ego} z parametrem $\theta$
        \[
            X \sim Bern(\theta)
        \]
        Wówczas
        \[
            \mu=\theta
        \]
        \[
            \sigma^2=\theta(1-\theta)
        \]
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{bernoulliego.png}
            \caption{Funkcja prawdopodobieństwa w rozkładzie
                Bernoulli'ego.}
        \end{figure}
    \end{definition}
    \begin{definition}[Dyskretny rozkład jednostajny]
        Jeśli $\mathcal{S}=\{x_1,x_2,\ldots,x_n\}$ i $p(x_i)=\frac{1}{n}$
        dla każdego $i \in \{1,2,\ldots,n\}$, to mówimy, że zmienna
        losowa ma \textbf{dyskretny rozkład jednostajny} na $n$ punktach.
        Wówczas
        \[
            \mu=\frac{1}{n}\sum_{i=1}^{n}x_i
        \]
        \[
            \sigma^2=\frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)^2
        \]
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]
                {jednostajny_dyskretny.png}
            \caption{Funkcja prawdopodobieństwa w dyskretnym rozkładzie
                jednostajnym.}
        \end{figure}
    \end{definition}
    \begin{definition}[Próba Bernoulli'ego]
        Rozważmy doświadczenie losowe o dwu możliwych wynikach:
        \begin{itemize}
            \item \textbf{sukces} z prawdopodobieństwem
                $\theta \in (0,1)$
            \item \textbf{porażka} z prawdopodobieństwem $1-\theta$
        \end{itemize}
        Doświadczenie tego rodzaju nazywamy
        \textbf{próbą Bernoulli'ego}.
    \end{definition}
    \begin{definition}[Schemat dwumianowy]
        Schemat doświadczenia określony jako n-krotne powtórzenie próby
        Bernoulli'ego w ten sposób, że poszczególne próby są niezależne.
        Długość schematu może być skończona lub nieskończona.
    \end{definition}
    \begin{definition}[Rozkład dwumianowy]
        Niech $X$ będzie zmienną losową, której wartością jest liczba
        sukcesów w schemacie dwumianowym o długości $n$ z prawdopodobieństwem
        sukcesu $\theta$. Wówczas $X$ ma rozkład dyskretny, w którym
        \[
            \mathcal{S}=\{0,1,\ldots,n\}
        \]
        oraz
        \[
            p(k)=\binom{n}{k}\theta^k(1-\theta)^{n-k}
        \]
        Jeśli zmienna $X$ ma rozkład dwumianowy o parametrach $n\in \mathbb{N}$
        i $\theta \in (0,1)$, to zapisujemy
        \[
            X \sim Binom(n,\theta)
        \]
        oraz
        \[
            \mu=n\theta
        \]
        \[
            \sigma^2=n\theta(1-\theta)
        \]
        Łatwo zauważyć, że
        \[
            Binom(1,\theta)=Bern(\theta).
        \]
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]
                {dwumianowy.png}
            \caption{Funkcja prawdopodobieństwa w rozkładzie dwumianowym.}
        \end{figure}
    \end{definition}
    \begin{definition}[Rozkład geometryczny]
        Zmienna losowa $T$ ma rozkład geometryczny z parametrem
        $\theta \in (0,1)$
        \[
            T \sim Geom(\theta),
        \]
        jeśli
        \[
            \mathcal{S}=\mathbb{N}\setminus \{0\},
        \]
        a funkcja prawdopodobieństwa ma postać
        \[
            p(k)=(1-\theta)^{k-1}\theta
        \]
        \[
            k\in\mathcal{S}
        \]
        Zmienna $T$ opisuje czas oczekiwania na pierwszy sukces w schemacie
        dwumianowym (o nieskończonej długości). Wówczas
        \[
            \mu=\frac{1}{\theta}
        \]
        \[
            \sigma^2=\frac{1-\theta}{\theta^2}
        \]
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]
                {geometryczny.png}
            \caption{Funkcja prawdopodobieństwa w rozkładzie geometrycznym.}
        \end{figure}
    \end{definition}
    \begin{definition}[Rozkład Poissona]
        Jeśli zmienna $N$ o wartościach w $N$ opisuje liczbę wystąpień
        pewnego powtarzalnego zdarzenia w przedziale czasowym $[0,t]$, przy
        czym spełnione są założenia:
        \begin{itemize}
            \item powtórzenia zdarzenia występują niezależnie od siebie,
            \item intensywność wystąpień $r>0$, czyli średnia liczba
                wystąpień w jednostce czasu jest stała,
            \item w danej chwili może zajść co najwyżej jedno powtórzenie,
        \end{itemize}
        to zmienna ma rozkład Poissona z parametrem $\lambda=rt$
        \[
            N \sim Pois(\lambda).
        \]
        Wówczas
        \[
            \mathcal{S}=\mathbb{N} \cup \{0\}
        \]
        \[
            p(k)=\frac{e^{-\lambda}\lambda^k}{k!}
        \]
        \[
            k\in \mathcal{S}
        \]
        oraz
        \[
            \mu=\lambda
        \]
        \[
            \sigma^2=\lambda
        \]
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]
                {poissona.png}
            \caption{Funkcja prawdopodobieństwa w rozkładzie Poissona.}
        \end{figure}
    \end{definition}
    \begin{theorem}[Twierdzenie Poissona]
        Niech $X_n$ będzie ciągiem zmiennych losowych takich, że
        \[
            X_n \sim Binom(n,\theta_n),
        \]
        gdzie $\theta_n$ jest takim ciągiem, że
        \[
            \lim_{n \to \infty} n\theta_n=\lambda
        \]
        dla pewnej liczby $\lambda>0$. Wówczas
        \[
            \lim_{n \to \infty} P(X_n=k)=\frac{e^{-\lambda}\lambda^k}{k!}.
        \]
    \end{theorem}

    \section{Literatura}
    \begin{thebibliography}{9}
        \bibitem{Smołka} Smołka, M. (2025). Rachunek prawdopodobieństwa
        i statystyka. \textit{Wykłady prowadzone na Akademii
        Górniczo-Hutniczej w Krakowie}.
    \end{thebibliography}
\end{document}
